# Dynamic Programming Problems

---

## Practical Limits: When Each Approach Becomes Infeasible

Before implementing any knapsack solution, understand the real-world limits. Below are **order-of-magnitude thresholds** assuming:
- Single modern CPU
- Seconds to minutes of runtime
- Reasonably optimized code
- Exact solution (unless noted)

### 1Ô∏è‚É£ Brute Force (Try All Subsets)

**Complexity:** O(2‚Åø)

**Practical Limits:**

| Items (n) | Feasible? | Time Estimate | Why |
|-----------|-----------|---------------|-----|
| ‚â§ 20 | ‚úÖ | < 1 second | ~1 million subsets |
| 25 | ‚úÖ | ~1 second | ~33 million subsets |
| 30 | ‚ö†Ô∏è | ~30 seconds | ~1 billion subsets (borderline) |
| 35 | ‚ùå | ~18 minutes | 34 billion subsets |
| 40 | ‚ùå | ~18 hours | 1 trillion subsets |

**Rule of Thumb:** Brute force dies hard around **30 items**

**When to Use:**
- ‚úÖ Interview proof-of-concept
- ‚úÖ Educational purposes
- ‚ùå Production (except very small inputs)

---

### 2Ô∏è‚É£ Dynamic Programming (Capacity-Based)

**Complexity:** O(n √ó capacity)

**Limiting Factor:** Capacity size matters MORE than items

**Practical Limits:**

| Items | Capacity | Total Ops | Feasible? |
|-------|----------|-----------|-----------|
| 100 | 10‚Å¥ | 10‚Å∂ | ‚úÖ Fast |
| 1,000 | 10‚Åµ | 10‚Å∏ | ‚úÖ Acceptable |
| 10,000 | 10‚Åµ | 10‚Åπ | ‚ö†Ô∏è Slow but works |
| Any | 10‚Å∂ | ~10‚Åπ | ‚ö†Ô∏è Slow |
| Any | 10‚Å∑ | ~10¬π‚Å∞ | ‚ùå Too slow |
| Any | ‚â• 10‚Å∏ | Massive | ‚ùå Out of memory |

**Memory Constraints:**

```
Memory = n √ó capacity √ó 8 bytes (for 64-bit integers)

1,000 items, capacity 10‚Åµ: 800 MB ‚úÖ
10,000 items, capacity 10‚Åµ: 8 GB ‚ö†Ô∏è
1,000 items, capacity 10‚Å∂: 8 GB ‚ö†Ô∏è
10,000 items, capacity 10‚Å∂: 80 GB ‚ùå
```

**Rule of Thumb:** DP works well when **capacity ‚â§ 10‚Å∂** (ideally ‚â§ 10‚Åµ)

**Why it's called "Pseudo-Polynomial":**
- Polynomial in n and W separately
- But W can be exponentially large compared to n
- So overall not truly polynomial

**When to Use:**
- ‚úÖ Small to medium capacity (‚â§ 10‚Å∂)
- ‚úÖ Large number of items (n up to 10‚Å¥)
- ‚ùå Huge capacities (W ‚â• 10‚Å∑)

---

### 3Ô∏è‚É£ Greedy (Ratio-Based)

**Complexity:** O(n log n)

**Practical Limits:**

| Items | Feasible? | Time |
|-------|-----------|------|
| 10‚Å∂ | ‚úÖ | < 1 second |
| 10‚Å∑ | ‚úÖ | ~10 seconds |
| 10‚Å∏ | ‚úÖ | ~100 seconds |
| 10‚Åπ | ‚ö†Ô∏è | Memory bound |

**Rule of Thumb:** Greedy scales **almost infinitely** (limited mainly by memory and sorting)

**Critical Caveat:**
- ‚ùå **NOT exact for 0/1 Knapsack** (can be 50% off)
- ‚úÖ **EXACT for Fractional Knapsack** (can take partial items)
- ‚úÖ Good for **approximation** when optimality not required

**When to Use:**
- ‚úÖ Fractional knapsack (exact)
- ‚úÖ 0/1 knapsack when approx is ok (and it must be quick)
- ‚ùå 0/1 knapsack when exact answer required

---

### 4Ô∏è‚É£ Branch and Bound

**Complexity:** Worst-case exponential, but often much faster in practice

**Very input-dependent** (depends on data and bounds tightness)

**Practical Limits:**

| Items | Best Case | Worst Case | Typical |
|-------|-----------|-----------|---------|
| 30 | ‚úÖ Fast | ‚úÖ OK | ‚úÖ Good |
| 40 | ‚úÖ Good | ‚ö†Ô∏è Slow | ‚úÖ Reasonable |
| 50 | ‚ö†Ô∏è Slow | ‚ùå Very slow | ‚ö†Ô∏è Borderline |
| 60 | ‚ùå Very slow | ‚ùå Impossible | ‚ùå Don't try |

**Works Best When:**
- Items are sorted by value/weight ratio
- Bounds are tight
- Data has structure you can exploit
- You can prune many branches early

**Rule of Thumb:** Exact, but **fragile** ‚Äî reliable up to ~**50 items**, dies around **60**

**When to Use:**
- ‚úÖ Interview when optimality matters
- ‚úÖ When you can tighten bounds
- ‚ö†Ô∏è Production (risky for large inputs)

---

### 5Ô∏è‚É£ Meet-in-the-Middle

**Complexity:** O(2^(n/2)) ‚Äî splits problem into two halves

**Practical Limits:**

| Items | Space | Time | Feasible? |
|-------|-------|------|-----------|
| 30 | 32 MB | < 1 sec | ‚úÖ |
| 40 | 512 MB | ~5 sec | ‚úÖ |
| 45 | ~4 GB | ~30 sec | ‚ö†Ô∏è |
| 50 | ~32 GB | ~5 min | ‚ö†Ô∏è |
| 55 | ~256 GB | ~1 hour | ‚ùå |

**Rule of Thumb:** Hard ceiling around **~50 items** (dominated by space, not time)

**When to Use:**
- ‚úÖ Interview: impressive and exact for ~40 items
- ‚úÖ When you need exact answer for medium-sized input
- ‚ùå Large inputs (memory explosion)

---

### 6Ô∏è‚É£ Approximation (FPTAS - Fully Polynomial Time Approximation)

**Complexity:** O(n¬≤ / Œµ) where Œµ = allowed error

**Practical Limits:**

| Items | Error | Time | Feasible? |
|-------|-------|------|-----------|
| 10‚Å¥ | 1% | < 1 sec | ‚úÖ |
| 10‚Åµ | 1% | ~10 sec | ‚úÖ |
| 10‚Å∂ | 1% | ~100 sec | ‚úÖ |
| 10‚Å∑ | 1% | ~1000 sec | ‚ö†Ô∏è |
| 10‚Åµ | 0.01% | ~100 sec | ‚ö†Ô∏è |
| 10‚Å∂ | 0.01% | ~1000 sec | ‚ö†Ô∏è |

**Accuracy Trade-off:**
```
1% error ‚Üí very fast
0.1% error ‚Üí slower
0.01% error ‚Üí slow
0.001% error ‚Üí very slow
```

**Rule of Thumb:** Scales **extremely well** if you allow small error (1-5%)

**When to Use:**
- ‚úÖ When exact is overkill
- ‚úÖ When you have millions of items
- ‚úÖ When 1-5% error is acceptable
- ‚ùå When you absolutely need optimal

---

### 7Ô∏è‚É£ Heuristics / Metaheuristics

**Examples:** Genetic algorithms, simulated annealing, ant colony optimization

**Complexity:** Depends on iterations, not n

**Practical Limits:**

| Items | Feasible? | Notes |
|-------|-----------|-------|
| 10‚Å¥ | ‚úÖ | Fast, good solutions |
| 10‚Åµ | ‚úÖ | Industry standard |
| 10‚Å∂ | ‚úÖ | Takes time but works |
| 10‚Å∑ | ‚ö†Ô∏è | Memory/iteration overhead |
| 10‚Å∏+ | ‚ùå | Rarely justifiable |

**Rule of Thumb:** **No optimality guarantee**, but often very good solutions in reasonable time

**When to Use:**
- ‚úÖ **Industry standard** for real knapsack problems
- ‚úÖ When large inputs and fast approximation needed
- ‚úÖ When you have time budget not accuracy budget
- ‚ùå When optimal is absolutely required

---

### 8Ô∏è‚É£ Integer Linear Programming (ILP)

**Using solvers:** CPLEX, Gurobi, CBC, SCIP

**Complexity:** NP-hard, but solvers are insanely optimized

**Practical Limits:**

| Binary Variables | Feasible? | Notes |
|------------------|-----------|-------|
| ‚â§ 1,000 | ‚úÖ | Fast, < 1 second |
| ‚â§ 5,000 | ‚ö†Ô∏è | 1-10 seconds |
| ‚â§ 10,000 | ‚ö†Ô∏è | 10-60 seconds |
| ‚â• 20,000 | ‚ùå | Exact is hard |

**Approximations/Relaxations:**
```
Linear Programming (relaxed): 100,000+ variables ‚úÖ
Cutting planes: 10,000-50,000 ‚úÖ
Exact (branch & cut): 5,000-10,000 ‚ö†Ô∏è
```

**Rule of Thumb:** Solvers are incredibly smart, but still NP-hard. Expect **1-5k variables** for reasonable solve time

**When to Use:**
- ‚úÖ When you have a solver available
- ‚úÖ Production systems with $ budget
- ‚úÖ Complex constraints (not just capacity)
- ‚ùå Interviews (too heavy-weight)

---

## üéØ Big Comparison Table (Bookmark This!)

| Method | Exact? | Dies At | Best For |
|--------|--------|---------|----------|
| **Brute Force** | ‚úÖ | ~30 items | Proof of concept, tiny inputs |
| **DP (Capacity)** | ‚úÖ | W ‚âà 10‚Å∂ | Classic interview, medium capacity |
| **Greedy** | ‚ùå | Never (wrong) | Fractional knapsack, approximation |
| **Branch & Bound** | ‚úÖ | ~50-60 items | Interview + tightable bounds |
| **Meet-in-Middle** | ‚úÖ | ~50 items | Interview (impresses people) |
| **FPTAS** | ‚ùå | 10‚Å∂+ items | When 1% error OK, huge inputs |
| **Heuristics** | ‚ùå | 10‚Å∏+ items | **Industry standard for real problems** |
| **ILP Solver** | ‚úÖ | 1k-5k vars | Production + complex constraints |

---

## üí° Quick Decision Guide

```
Input Size: n = 30?
‚îú‚îÄ YES ‚Üí Use DP or Branch & Bound (exact, reasonable time)
‚îî‚îÄ NO
    ‚îÇ
    Input Size: n = 50?
    ‚îú‚îÄ YES ‚Üí Use Meet-in-Middle or ILP solver
    ‚îî‚îÄ NO
        ‚îÇ
        Input Size: n = 1000?
        ‚îú‚îÄ YES ‚Üí Use DP (if capacity ‚â§ 10‚Å∂)
        ‚îî‚îÄ NO
            ‚îÇ
            Input Size: n = 100,000?
            ‚îú‚îÄ YES ‚Üí Use FPTAS or Heuristics
            ‚îî‚îÄ NO
                ‚îî‚îÄ Use Heuristics/Metaheuristics + ILP relaxation
```

---

## Traveling Salesman Problem (TSP) ‚Äî Practical Limits

### Why TSP is Harsher Than Knapsack

**Key Difference:**

- **Knapsack** ‚Üí Combinations (2‚Åø)
- **TSP** ‚Üí Permutations ((n‚àí1)!)

**Factorial growth is FAR worse than exponential.** That's why TSP collapses much earlier.

**Comparison:**
```
n = 20:
  2‚Åø = 1,048,576 (millions)
  n! = 2.4 √ó 10¬π‚Å∏ (quintillions) ‚Üê 1 TRILLION TIMES WORSE
```

---

### 1Ô∏è‚É£ Brute Force TSP (Try All Tours)

**Complexity:** O((n‚àí1)! / 2) with symmetry removed

**Practical Limits:**

| Cities (n) | Tours | Feasible? | Time Estimate |
|-----------|-------|-----------|---------------|
| 10 | ~180k | ‚úÖ | < 1 ms |
| 12 | ~20M | ‚ö†Ô∏è | ~1 second |
| 14 | ~3.1B | ‚ö†Ô∏è | ~30 seconds |
| 15 | ~43B | ‚ùå | ~10 minutes |
| 20 | ~60 quadrillion | ‚ò†Ô∏è | Never |

**Rule of Thumb:** Brute-force TSP **dies hard at ~12-14 cities**

**Comparison to Knapsack:**
- Knapsack brute force: ~30 items
- TSP brute force: ~12 cities
- **TSP dies 2.5x earlier**

---

### 2Ô∏è‚É£ DFS / Backtracking (Still Brute Force)

Just brute force with recursion.

**Improvement?** Slight pruning, but hits the same factorial wall

**Practical Limit:** ~15 cities max

**Why?** Without a good upper bound, can't prune effectively

---

### 3Ô∏è‚É£ Branch and Bound (Exact, Smarter)

**Idea:** DFS + prune paths that already exceed best known cost

**Practical Limits (highly input-dependent):**

| Cities | Feasible? | Notes |
|--------|-----------|-------|
| ‚â§ 20 | ‚úÖ | Good pruning possible |
| 25 | ‚ö†Ô∏è | Borderline, depends on data |
| 30 | ‚ùå | Bad distance distributions defeat pruning |
| ‚â• 35 | ‚ò†Ô∏è | Worst case is still factorial |

**Rule of Thumb:** Exact B&B dies around **25-30 cities**

**Why it fails:**
- Worst-case still factorial
- Bad pruning when cities are scattered randomly
- Real-world TSP often has structure that helps; random data is worst-case

---

### 4Ô∏è‚É£ Dynamic Programming (Held‚ÄìKarp Algorithm)

**Complexity:** O(n¬≤ √ó 2‚Åø)

**Memory:** O(n √ó 2‚Åø) - the killer constraint

**Practical Limits:**

| Cities | Memory | Time | Feasible? |
|--------|--------|------|-----------|
| 16 | 16 KB | < 1 sec | ‚úÖ |
| 18 | 64 KB | ~1 sec | ‚úÖ |
| 20 | 256 KB | ~5 sec | ‚úÖ |
| 22 | 1 MB | ~20 sec | ‚ö†Ô∏è |
| 24 | 4 MB | ~2 min | ‚ö†Ô∏è |
| 25 | 8 MB | ~5 min | ‚ö†Ô∏è |
| 26 | 16 MB | ~10 min | ‚ö†Ô∏è |
| 28 | 64 MB | ~1 hour | ‚ùå |

**Rule of Thumb:** Exact DP dies around **22-24 cities** (memory wall hits hard)

**Why it's famous:** Best general exact algorithm for small TSP, but still fundamentally exponential

---

### 5Ô∏è‚É£ Meet-in-the-Middle (Rarely Used for TSP)

Hard to apply cleanly to TSP structure.

Slight gains only.

**Practical Limit:** ~25 cities (theoretical, hard to implement well)

---

### 6Ô∏è‚É£ Approximation Algorithms (Guaranteed Bounds)

**Example:** Christofides Algorithm (for metric TSP)

- Always ‚â§ 1.5 √ó optimal
- Polynomial time
- Only works for metric TSP (triangle inequality holds)

**Practical Limits:**

| Cities | Feasible? |
|--------|-----------|
| 10‚Åµ | ‚úÖ |
| 10‚Å∂ | ‚úÖ |
| 10‚Å∑ | ‚úÖ |

**Rule of Thumb:** **Scales perfectly** if you accept 1.5√ó non-optimality

**Catch:** Only works for metric TSP, not arbitrary distances

---

### 7Ô∏è‚É£ Heuristics (Industry Standard)

**Examples:**
- Nearest Neighbor
- 2-opt / 3-opt local search
- Lin‚ÄìKernighan heuristic
- Simulated Annealing
- Genetic Algorithms
- Ant Colony Optimization

**Practical Limits:**

| Cities | Feasible? | Solution Quality |
|--------|-----------|------------------|
| 1,000 | ‚úÖ | Within 0.5-2% of optimal |
| 10,000 | ‚úÖ | Within 1-3% of optimal |
| 100,000 | ‚úÖ | Within 2-5% of optimal |
| 1,000,000 | ‚ö†Ô∏è | Time/memory intensive |

**Rule of Thumb:** **This is how logistics actually solves TSP**

**Why heuristics work:**
- Real-world TSP has structure (geographic clusters, highways)
- Random starting points ‚Üí hill climbing ‚Üí local optima ‚Üí often very good
- Can run multiple times and pick best

**Accuracy:** Often within 0.1-2% of optimal (no guarantee, but empirically excellent)

---

### 8Ô∏è‚É£ Integer Linear Programming (with Cutting Planes)

**Reality:** This is how world-record TSP instances are solved

**Achievements:**
- Exact solution for **85,900+ cities** (Concorde TSP Solver)
- But... months of computation
- Massive cutting plane generation
- Geometric structure exploited (Euclidean metric)

**Why this works:**
- Real maps ‚â† worst-case TSP
- Euclidean geometry saves you (most permutations eliminated by geometry)
- Cutting planes destroy infeasible regions
- Massive computational resources

**Practical Limits:**
- Without special structure: 1k-5k cities
- With Euclidean geometry: 10k-100k cities
- Record holders: 80k+ cities (special cases only)

---

## üéØ Big Comparison Table (TSP)

| Method | Exact? | Dies At | Best For |
|--------|--------|---------|----------|
| **Brute Force** | ‚úÖ | ~12-14 cities | Tiny inputs only |
| **DFS / Backtracking** | ‚úÖ | ~15 cities | Interview naive approach |
| **Branch & Bound** | ‚úÖ | ~25-30 cities | Interview + good pruning |
| **DP (Held‚ÄìKarp)** | ‚úÖ | ~22-24 cities | Exact for small TSP |
| **Meet-in-Middle** | ‚úÖ | ~25 cities | Theoretical (hard to implement) |
| **Approximation** | ‚ùå | ~10‚Å∂ cities | Guaranteed 1.5√ó optimal |
| **Heuristics** | ‚ùå | ~10‚Åµ-10‚Å∂ cities | **Industry standard** |
| **ILP + Cuts** | ‚úÖ | ~85k cities | Real-world with structure |

---

## üî• TSP vs Knapsack: The Key Contrast

| Aspect | Knapsack | TSP |
|--------|----------|-----|
| **Growth** | 2‚Åø | (n‚àí1)! |
| **Exact Limit** | ~50 items | ~25 cities |
| **Why** | Combinations | Permutations |
| **Approx scales to** | Very well (10‚Å∂+) | Very well (10‚Å∂+) |
| **Industry use** | Approx / ILP | **Heuristics / ILP** |
| **Worst-case** | Hopeless | Impossibly hopeless |
| **Real-world** | Structured ‚Üí approx works | Structured ‚Üí heuristics work |

---

## üí° One-Sentence Takeaway (Remember This)

**"Exact TSP dies around 25 cities; practical TSP scales to hundreds of thousands by giving up perfection."**

---

## Why Quantum Computers Don't Save TSP

This is important: there's a lot of hype around quantum computing and NP-hard problems. Let me explain what's real and what's myth.

### The Short Answer (No Hype)

**Quantum computers do not:**
- ‚ùå Magically try all TSP routes at once
- ‚ùå Read out exponentially many answers
- ‚ùå Turn factorial-time problems into polynomial-time ones

**Therefore:** Quantum doesn't save TSP.

---

### Myth to Kill First: "Quantum Tries All Possibilities in Parallel"

**The claim:** Quantum superposition lets us check all routes simultaneously.

**The reality:** A quantum system can represent many states at once, but **when you measure, you get ONE answer**. The other states vanish.

**Real-world analogy üé≠**

Imagine:
1. **1 trillion people silently try different routes**
2. **You're allowed to ask ONE of them for their answer**
3. **The rest vanish instantly**

That's quantum measurement.

**Parallelism without readable output is useless** unless you can amplify the right answer through interference.

---

### Why Quantum Helps SOME Problems (But Not TSP)

Quantum gives genuine speedups when:

‚úÖ **You can interfere paths** ‚Üí wrong answers cancel out, right answers amplify
‚úÖ **Strong mathematical structure** ‚Üí global properties you can exploit
‚úÖ **Specific goal states** ‚Üí interference naturally amplifies them

**Examples where quantum helps:**
- Factoring large numbers (Shor's algorithm)
- Searching unsorted databases (Grover's algorithm)
- Problems with clear symmetry

**TSP does NOT have this structure:**
- ‚ùå No interference pattern among routes
- ‚ùå No global property to amplify
- ‚ùå No mathematical shortcut

---

### What Quantum CAN Do: Grover's Algorithm

Grover's algorithm gives a **quadratic speedup**.

**Classic search:** N possibilities take N time (try them all)

**Quantum search:** N possibilities take ‚àöN time

**For TSP with brute force:**

| Approach | Time |
|----------|------|
| Classical brute force | (50‚àí1)! ‚âà 10¬≥¬≤ |
| Quantum brute force | ‚àö((50‚àí1)!) ‚âà 10¬π‚Å∂ |

**Result:** Still hopeless. You can't compute 10¬π‚Å∂ operations in reasonable time.

**Key insight:** Quadratic speedup ‚â† salvation when dealing with factorials.

---

### Why You Can't "Encode Shortest Route" Cleverly

**The idea:** Use quantum interference to amplify only the shortest route.

**The problem:** To do that, you'd need to:

1. Compare all route lengths somehow
2. Without measuring them (which collapses superposition)
3. Amplify only the optimal one

**But here's the catch:**

Comparing route lengths **already requires evaluating them**, which is the hard part.

You can't get quantum interference "for free" ‚Äî you'd still have to do the exponential work.

**No free lunch:** Quantum doesn't bypass the fundamental hard work.

---

### The Measurement Wall üöß

**The hard limit:**

Quantum computers **cannot:**
- ‚ùå Output exponential amounts of information
- ‚ùå Reveal which of exponentially many states was best
- ‚ùå Collapse to the optimal solution deterministically

**Why?** Information theory forbids it.

**Intuition:** If you could extract exponentially many bits of information from a polynomial-sized system, you could solve any NP problem easily. But that's believed false.

---

### What Theorists Actually Know

**Proven facts:**

- Grover's algorithm is optimal for unstructured search
- No quantum algorithm is known to solve NP-complete problems efficiently
- Most complexity theorists believe:

```
NP ‚äà BQP

(NP problems cannot all be solved efficiently by quantum computers)
```

**In plain English:**
Even with quantum computers, we don't know how to efficiently solve NP-complete problems like TSP.

---

### Real-World Quantum Optimization (What Actually Happens)

**How quantum is used in practice:**

Quantum is treated as **another heuristic**, like simulated annealing:

#### Quantum Annealing (D-Wave)
- Finds good (not optimal) routes
- Sometimes helps, sometimes doesn't
- No guarantees
- Scales poorly

#### QAOA (Quantum Approximate Optimization Algorithm)
- Designed for combinatorial problems
- Empirically mixed results
- Often no better than classical heuristics

**Critical point:** These do **not break NP-completeness**. They're just another hill-climbing technique.

---

### Why Geometry Beats Quantum

**Here's a key insight that many miss:**

```
Classical heuristics exploit Euclidean geometry.
Quantum algorithms do not get extra leverage from geometry.
Geometry helps classical methods MORE than quantum helps.
```

**Real-world TSP has:**
- Cities in space (Euclidean metric)
- Triangle inequality (distances are consistent)
- Geographic clustering (cities group together)

**Classical algorithms exploit all of this:**
- Nearest neighbor works ~1% off optimal because clusters exist
- 2-opt local search finds great routes by respecting geography
- Lin‚ÄìKernighan uses spatial structure brilliantly

**Quantum doesn't get any of these advantages.** A quantum computer can't "see" that cities form clusters.

---

### The Core Reason (This is Inevitable)

**TSP is hard because:**

There are too many distinct answers, and **quantum mechanics cannot let you read exponentially many answers**.

**That's the brick wall.**

You cannot:
1. Create a superposition of all solutions
2. Extract which one is best
3. Without this step being the exponential work

Superposition without readout is a magic trick, not computation.

---

### Why Quantum Won't Save You in 10 Years Either

Even if quantum computers get much better:

**Scaling problems:**
- Quantum systems are extremely fragile
- Error rates scale with problem size
- Creating/maintaining superposition gets harder
- Temperature requirements keep dropping

**Complexity barriers:**
- Even with perfect quantum hardware, the algorithm limits remain
- You still can't extract exponential information
- You still can't bypass combinatorial explosion

**The honest truth:**
Quantum might be useful for some problems, but TSP is not one of them.

---

## üéØ Quantum vs Classical for NP-Hard Problems

| Aspect | Classical | Quantum |
|--------|-----------|---------|
| **Brute Force Speedup** | Baseline | ‚àö(2‚Åø) quadratic |
| **Heuristics** | **Very effective** | Marginal |
| **Exploits Geometry** | **Yes** | No |
| **Solves TSP Exactly** | Up to ~25 cities | Up to ~25 cities |
| **Practical Use** | Industry standard | Research only |
| **Promise vs Reality** | Matches | Hype > Reality |

---

## üí° One-Liner You Can Use

**"Quantum computers give quadratic speedup on brute force, but that's not enough when you're dealing with factorials. For TSP, classical heuristics actually outperform quantum approaches."**

Or even shorter:

**"Quantum doesn't turn exponential into polynomial. For TSP, geometry beats physics."**

---

## 0/1 Knapsack Problem

### Problem Statement

You are given:
- **n** items
- Each item has:
  - `wt[i]` ‚Üí weight
  - `val[i]` ‚Üí value
- A knapsack with maximum capacity **W**

**Goal:** Find the maximum total value that can be put in the knapsack without exceeding weight W.

**Constraint:** Each item can be chosen at most once (0/1 choice).

### Why Dynamic Programming?

DP is used because:
- Subproblems repeat
- Optimal solution depends on optimal solutions of smaller subproblems
- Overlapping subproblems exist

### DP State Definition

```
dp[i][w] = maximum value using first i items with weight limit w
```

### DP Transition (MOST IMPORTANT)

For each item `i` and weight `w`, we have two choices:

#### Option 1: Do NOT take item i
```
dp[i][w] = dp[i-1][w]
```

**Meaning:**
- Skip the current item
- Best value remains what we already computed using previous items

#### Option 2: Take item i (only if wt[i] ‚â§ w)
```
dp[i][w] = val[i] + dp[i-1][w - wt[i]]
```

**Meaning:**
- Add value of current item
- Reduce remaining weight
- Use previous items only (0/1 constraint)

#### Final Recurrence
```
dp[i][w] = max(
    dp[i-1][w],                              # do not take
    val[i] + dp[i-1][w - wt[i]]            # take item
)
```

### Base Cases

```
dp[0][w] = 0    # no items ‚Üí no value
dp[i][0] = 0    # no capacity ‚Üí no value
```

### Python Code (2D DP)

```python
def knapsack(wt, val, W):
    n = len(wt)
    dp = [[0] * (W + 1) for _ in range(n + 1)]

    for i in range(1, n + 1):
        for w in range(W + 1):
            # Case 1: Do not take item i
            dp[i][w] = dp[i - 1][w]

            # Case 2: Take item i (if possible)
            if wt[i - 1] <= w:
                dp[i][w] = max(
                    dp[i][w],
                    val[i - 1] + dp[i - 1][w - wt[i - 1]]
                )

    return dp[n][W]
```

### Special Explanations

#### Why dp[i-1][w]?
```python
dp[i][w] = dp[i - 1][w]
```

If we skip the current item, we reuse the best solution from previous items for the same weight.

#### Why val[i-1] + dp[i-1][w - wt[i-1]]?
```python
val[i - 1] + dp[i - 1][w - wt[i - 1]]
```

If we take the item:
- Add its value
- Reduce remaining weight
- Use only previous items (0/1 rule)

#### Why max(...)?
```python
dp[i][w] = max(not_take, take)
```

We always choose the best of the two choices.

### Example Walkthrough

```
Items:    wt = [2, 3, 4, 5]
          val = [3, 4, 5, 6]
Capacity: W = 5

dp[0][w] = 0 (base case)

For item 1 (wt=2, val=3):
  dp[1][0] = 0
  dp[1][1] = 0 (can't fit)
  dp[1][2] = max(dp[0][2], 3 + dp[0][0]) = max(0, 3) = 3
  dp[1][3] = max(dp[0][3], 3 + dp[0][1]) = max(0, 3) = 3
  dp[1][4] = 3, dp[1][5] = 3

For item 2 (wt=3, val=4):
  dp[2][5] = max(dp[1][5], 4 + dp[1][2]) = max(3, 4+3) = 7
  ...

Final: dp[4][5] = maximum value with capacity 5
```

### Complexity Analysis

| Metric | Value |
|--------|-------|
| **Time Complexity** | O(n √ó W) |
| **Space Complexity** | O(n √ó W) for 2D DP, O(W) for 1D DP |

Where:
- n = number of items
- W = knapsack capacity

### Space-Optimized Version (1D DP)

```python
def knapsack_1d(wt, val, W):
    dp = [0] * (W + 1)

    for i in range(len(wt)):
        # Traverse from right to left to avoid using updated values
        for w in range(W, wt[i] - 1, -1):
            dp[w] = max(dp[w], val[i] + dp[w - wt[i]])

    return dp[W]
```

**Why reverse iteration?** We must process weights from right to left to avoid using the same item twice.

### Pseudo-Polynomial Time Complexity

‚ùó **Important:** 0/1 Knapsack is **pseudo-polynomial**, not truly polynomial.

**What does this mean?**

An algorithm is pseudo-polynomial if its running time is polynomial in the numeric value of the input, not in the size of the input (number of bits).

#### Example
```
W = 1,000,000
Input size (bits) = log‚ÇÇ(1,000,000) ‚âà 20 bits

Algorithm time = O(n √ó W) = O(n √ó 1,000,000)
This is NOT polynomial in input size (bits)
```

**Conclusion:** This DP solution works only when W is reasonably small.

### Key Insights

1. **State Representation:** `dp[i][w]` captures all necessary information
2. **Optimal Substructure:** Solution uses optimal solutions of subproblems
3. **Non-overlapping Choice:** We explicitly choose to take or not take each item
4. **Bottom-up Approach:** We build from smaller problems to larger ones

---

## Minimum Coins Problem

### Problem Statement

Given:
- An array of coin denominations
- A target amount

**Goal:** Find the minimum number of coins needed to make the target amount.

### Examples

```python
coins = [1, 2, 5]
amount = 13

Possible combinations:
- 13 √ó 1 = 13 coins
- 6 √ó 2 + 1 √ó 1 = 7 coins
- 2 √ó 5 + 3 √ó 1 = 5 coins (but using 2 and 5 better)
- 2 √ó 5 + 1 √ó 2 + 1 √ó 1 = 4 coins
- 1 √ó 5 + 4 √ó 2 = 5 coins

Minimum: 4 coins (5 + 5 + 2 + 1)
```

### DP Approach

**State:** `dp[i]` = minimum coins needed to make amount i

**Transition:** For each coin, update all amounts that can be formed:
```
dp[amount] = min(dp[amount], dp[amount - coin] + 1)
```

### Python Solution

```python
def find_minimum_coins(coins, amount):
    # dp[i] = minimum coins to make amount i
    dp = [float("inf")] * (amount + 1)
    dp[0] = 0  # base case: 0 coins for amount 0

    for i in range(1, amount + 1):
        for coin in coins:
            if i >= coin:
                dp[i] = min(dp[i], dp[i - coin] + 1)

    return dp[amount] if dp[amount] != float("inf") else -1

# Example
result = find_minimum_coins([1, 2, 5], 13)
print(result)  # Output: 4
```

### With Path Reconstruction

```python
def find_minimum_coins_with_path(coins, amount):
    dp = [float("inf")] * (amount + 1)
    parent = [-1] * (amount + 1)
    dp[0] = 0

    for i in range(1, amount + 1):
        for coin in coins:
            if i >= coin and dp[i - coin] + 1 < dp[i]:
                dp[i] = dp[i - coin] + 1
                parent[i] = coin

    # Reconstruct path
    path = []
    current = amount
    while current > 0:
        coin = parent[current]
        path.append(coin)
        current -= coin

    return dp[amount], path
```

### Complexity Analysis

| Metric | Value |
|--------|-------|
| **Time** | O(amount √ó n) where n = number of coins |
| **Space** | O(amount) |

### Key Differences from 0/1 Knapsack

| Feature | 0/1 Knapsack | Coin Change |
|---------|--------------|-------------|
| **Items** | Limited (each once) | Unlimited (each multiple times) |
| **DP Loop** | Iterate items then weights | Iterate amounts then coins |
| **Weight Constraint** | Maximum weight W | Exact amount |
| **Iteration Order** | Can be forward | Must be forward (allowing reuse) |

### Interview Tips

1. **Unbounded vs 0/1:** Coin change is unbounded (can use coins multiple times)
2. **Greedy Won't Work:** Greedy approach (always pick largest coin) fails for some coin sets
3. **Base Case:** Always initialize `dp[0] = 0`
4. **Impossible Cases:** Check if result is still infinity (impossible amount)

---

## Summary Comparison

| Problem | Constraint | Approach | Use Case |
|---------|-----------|----------|----------|
| **0/1 Knapsack** | Each item once | 2D DP with careful transition | Item selection, project selection |
| **Coin Change** | Coins unlimited | 1D DP, forward iteration | Making change, combinations |
| **DP Key** | Identify state | Subproblems overlap | Optimization problems |
